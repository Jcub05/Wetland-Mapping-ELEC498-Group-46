{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wetland Mapping - Dataset Preparation\n",
    "This notebook prepares the balanced dataset for wetland classification using Google Earth Engine embeddings.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload all your Google Earth TIF files to Kaggle dataset (55 files from `Google_Dataset/`)\n",
    "2. Upload `bow_river_wetlands_10m_final.tif` (wetland labels)\n",
    "3. Run this notebook to create the balanced 1.5M sample dataset\n",
    "4. Download the output `wetland_dataset_1.5M.npz` for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q rasterio tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository (gee_embed_CNN_dev branch)\n",
    "!git clone -b gee_embed_CNN_dev https://github.com/Jcub05/Wetland-Mapping-ELEC498-Group-46.git\n",
    "%cd Wetland-Mapping-ELEC498-Group-46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Organize Your Uploaded Files\n",
    "Move your uploaded TIF files to the expected locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Kaggle datasets are typically mounted at /kaggle/input/\n",
    "# Adjust these paths based on how you name your Kaggle dataset\n",
    "KAGGLE_INPUT = '/kaggle/input/wetland-embeddings'  # Change to match your dataset name\n",
    "\n",
    "# Create Google_Dataset directory if it doesn't exist\n",
    "os.makedirs('Google_Dataset', exist_ok=True)\n",
    "\n",
    "# Copy all embedding TIF files\n",
    "print(\"Copying embedding tiles...\")\n",
    "for file in os.listdir(KAGGLE_INPUT):\n",
    "    if file.endswith('.tif') and 'embeddings' in file:\n",
    "        src = os.path.join(KAGGLE_INPUT, file)\n",
    "        dst = os.path.join('Google_Dataset', file)\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"  Copied {file}\")\n",
    "\n",
    "# Copy labels file\n",
    "labels_src = os.path.join(KAGGLE_INPUT, 'bow_river_wetlands_10m_final.tif')\n",
    "if os.path.exists(labels_src):\n",
    "    shutil.copy(labels_src, 'bow_river_wetlands_10m_final.tif')\n",
    "    print(\"âœ“ Copied labels file\")\n",
    "else:\n",
    "    print(\"âš  Warning: Labels file not found. Make sure it's uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build VRT (Virtual Raster)\n",
    "Combine all 55 tiled TIF files into a single virtual raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the VRT builder from your repo\n",
    "!python build_vrt_and_verify.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Balanced Dataset\n",
    "Run the optimized dataloader to create ~1.5M balanced samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load files\n",
    "embeddings_file = \"bow_river_embeddings_2020_matched.vrt\"\n",
    "labels_file = \"bow_river_wetlands_10m_final.tif\"\n",
    "\n",
    "print(\"Loading labels...\")\n",
    "with rasterio.open(labels_file) as labels_src:\n",
    "    labels_full = labels_src.read(1)\n",
    "    print(f\"Labels (original): {labels_full.shape}\")\n",
    "\n",
    "print(f\"\\nOpening embeddings VRT: {embeddings_file}\")\n",
    "embeddings_src = rasterio.open(embeddings_file)\n",
    "print(f\"Embeddings: {embeddings_src.count} bands x {embeddings_src.height} x {embeddings_src.width}\")\n",
    "\n",
    "# Crop labels to match embeddings\n",
    "labels = labels_full[:embeddings_src.height, :embeddings_src.width]\n",
    "print(f\"Labels (cropped): {labels.shape}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert (embeddings_src.height, embeddings_src.width) == labels.shape, \"Dimension mismatch!\"\n",
    "print(\"âœ“ Dimensions match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "valid_mask = (labels >= 0) & (labels <= 5)\n",
    "valid_count = valid_mask.sum()\n",
    "print(f\"\\nTotal labeled pixels: {valid_count:,} out of {labels.size:,} ({100*valid_count/labels.size:.2f}%)\")\n",
    "\n",
    "unique_classes, class_counts = np.unique(labels[valid_mask], return_counts=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"  Class {cls}: {count:,} pixels ({100*count/valid_count:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced sampling strategy (~1.5M samples)\n",
    "samples_per_class = {\n",
    "    0: 600_000,   # Background\n",
    "    1: 19_225,    # Use ALL (smallest class)\n",
    "    2: 150_000,   # Moderate wetland type\n",
    "    3: 500_000,   # Largest wetland class\n",
    "    4: 150_000,   # Moderate wetland type\n",
    "    5: 100_000,   # Moderate wetland type\n",
    "}\n",
    "total_target = sum(samples_per_class.values())\n",
    "print(f\"\\nBalanced sampling strategy (target: {total_target:,} samples)\")\n",
    "\n",
    "sampled_indices_y = []\n",
    "sampled_indices_x = []\n",
    "sampled_labels = []\n",
    "\n",
    "for cls in unique_classes:\n",
    "    class_mask = (labels == cls)\n",
    "    y_idx, x_idx = np.where(class_mask)\n",
    "    \n",
    "    n_available = len(y_idx)\n",
    "    n_target = samples_per_class[cls]\n",
    "    n_sample = min(n_target, n_available)\n",
    "    \n",
    "    # Sample from this class\n",
    "    if n_available > n_target:\n",
    "        sample_idx = np.random.choice(n_available, n_target, replace=False)\n",
    "    else:\n",
    "        sample_idx = np.arange(n_available)\n",
    "        print(f\"  âš  Class {cls}: only {n_available:,} available (target: {n_target:,})\")\n",
    "    \n",
    "    sampled_indices_y.append(y_idx[sample_idx])\n",
    "    sampled_indices_x.append(x_idx[sample_idx])\n",
    "    sampled_labels.append(np.full(n_sample, cls))\n",
    "    \n",
    "    print(f\"  Class {cls}: sampled {n_sample:,} / {n_available:,} pixels\")\n",
    "\n",
    "# Combine and shuffle\n",
    "y_indices = np.concatenate(sampled_indices_y)\n",
    "x_indices = np.concatenate(sampled_indices_x)\n",
    "y = np.concatenate(sampled_labels)\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffle_idx = np.random.permutation(len(y_indices))\n",
    "y_indices = y_indices[shuffle_idx]\n",
    "x_indices = x_indices[shuffle_idx]\n",
    "y = y[shuffle_idx]\n",
    "\n",
    "print(f\"\\nTotal balanced samples: {len(y):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for loss function\n",
    "unique_sampled, sampled_counts = np.unique(y, return_counts=True)\n",
    "class_weights = torch.zeros(6)\n",
    "for cls, count in zip(unique_sampled, sampled_counts):\n",
    "    class_weights[cls] = 1.0 / count\n",
    "class_weights = class_weights / class_weights.sum() * 6\n",
    "\n",
    "print(\"\\nClass weights for loss function:\")\n",
    "for cls in range(6):\n",
    "    print(f\"  Class {cls}: {class_weights[cls]:.4f}\")\n",
    "print(\"\\nðŸ’¡ Use: nn.CrossEntropyLoss(weight=class_weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Embeddings (Optimized Batch Reading)\n",
    "This uses row-based batching for ~100x speedup over pixel-by-pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings using optimized batching\n",
    "print(\"\\nReading embeddings for sampled pixels (optimized batching)...\")\n",
    "n_samples = len(y_indices)\n",
    "X = np.zeros((n_samples, embeddings_src.count), dtype=np.float32)\n",
    "\n",
    "# Group samples by row for efficient batch reading\n",
    "row_to_samples = defaultdict(list)\n",
    "for idx, (y_coord, x_coord) in enumerate(zip(y_indices, x_indices)):\n",
    "    row_to_samples[y_coord].append((idx, x_coord))\n",
    "\n",
    "print(f\"Grouped {n_samples:,} samples into {len(row_to_samples):,} unique rows\")\n",
    "\n",
    "# Read row by row\n",
    "sample_count = 0\n",
    "with tqdm(total=len(row_to_samples), desc=\"Reading rows\", unit=\" rows\") as pbar:\n",
    "    for row_idx in sorted(row_to_samples.keys()):\n",
    "        # Read entire row at once\n",
    "        row_data = embeddings_src.read(window=((row_idx, row_idx+1), (0, embeddings_src.width)))\n",
    "        row_data = row_data[:, 0, :]  # (64, width)\n",
    "        \n",
    "        # Extract samples from this row\n",
    "        for sample_idx, col_idx in row_to_samples[row_idx]:\n",
    "            X[sample_idx, :] = row_data[:, col_idx]\n",
    "            sample_count += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "embeddings_src.close()\n",
    "print(f\"\\nâœ“ Loaded {sample_count:,} samples\")\n",
    "print(f\"  X shape: {X.shape} ({X.nbytes / (1024**3):.2f} GB)\")\n",
    "print(f\"  y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Dataset\n",
    "Save the prepared dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "output_file = 'wetland_dataset_1.5M.npz'\n",
    "np.savez_compressed(\n",
    "    output_file,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    class_weights=class_weights.numpy(),\n",
    "    samples_per_class=np.array(list(samples_per_class.values()))\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset saved to {output_file}\")\n",
    "print(f\"  File size: {os.path.getsize(output_file) / (1024**3):.2f} GB\")\n",
    "print(f\"\\nTo load for training:\")\n",
    "print(\"  data = np.load('wetland_dataset_1.5M.npz')\")\n",
    "print(\"  X, y = data['X'], data['y']\")\n",
    "print(\"  class_weights = torch.from_numpy(data['class_weights'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Quick Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved dataset\n",
    "data = np.load(output_file)\n",
    "print(\"Dataset contents:\")\n",
    "print(f\"  X: {data['X'].shape} - embeddings\")\n",
    "print(f\"  y: {data['y'].shape} - labels\")\n",
    "print(f\"  class_weights: {data['class_weights'].shape}\")\n",
    "print(f\"\\nClass distribution in dataset:\")\n",
    "unique, counts = np.unique(data['y'], return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"  Class {cls}: {count:,} samples ({100*count/len(data['y']):.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
