{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wetland Training Dataset — Smart Spatial Split\n",
        "\n",
        "**Output:** `wetland_dataset_smart_split.npz`\n",
        "\n",
        "This notebook fixes the **spatial data leakage** issue using a **data-driven test region**\n",
        "instead of an arbitrary geographic fraction.\n",
        "\n",
        "## Why a column-based (east/west) split?\n",
        "\n",
        "Analysis of the labels raster (`bow_river_wetlands_10m_final.tif`) revealed that the\n",
        "rare wetland classes are geographically concentrated in the **western (left) portion** of the study area:\n",
        "\n",
        "| Class | Pixel count | Col range |\n",
        "|-------|------------|----------|\n",
        "| Class 1 (rare) | 19,225 | cols 1,000 – 6,311 |\n",
        "| Class 2 | 901,620 | cols 747 – 7,037 |\n",
        "\n",
        "A row-based (north/south) split completely missed these classes in the test region.\n",
        "This notebook holds out all tiles with column offset **< `TEST_COL_MAX`** (≈ left 22% of the map),\n",
        "which is guaranteed to include every pixel of Classes 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1: Setup\n",
        "print('Setting up environment...')\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('Drive already mounted')\n",
        "\n",
        "!pip install -q rasterio tqdm\n",
        "\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Configuration\n",
        "print('=' * 70)\n",
        "print('CONFIGURATION')\n",
        "print('=' * 70)\n",
        "\n",
        "labels_file    = '/content/drive/MyDrive/bow_river_wetlands_10m_final.tif'\n",
        "embeddings_dir = Path('/content/drive/MyDrive/EarthEngine')\n",
        "output_file    = '/content/drive/MyDrive/wetland_dataset_smart_split.npz'\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# SMART SPLIT: Hold out tiles whose column offset is LESS than this value.\n",
        "#\n",
        "# This value is chosen to fully enclose the rare wetland classes:\n",
        "#   Class 1: max col = 6,311\n",
        "#   Class 2: max col = 7,037\n",
        "#\n",
        "# TEST_COL_MAX = 8,192 adds ~1,155 col pixels of buffer past Class 2's edge.\n",
        "# This captures all Class 1 and Class 2 pixels in the test set.\n",
        "# (Approx 22% of the 31,427-wide map.)\n",
        "TEST_COL_MAX = 8192\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# Per-class sample budgets\n",
        "train_samples_per_class = {\n",
        "    0: 600_000,\n",
        "    1: 19_225,\n",
        "    2: 150_000,\n",
        "    3: 500_000,\n",
        "    4: 150_000,\n",
        "    5: 100_000,\n",
        "}\n",
        "# Test budget: 25% of training budget per class\n",
        "test_samples_per_class = {cls: max(1000, int(n * 0.25)) for cls, n in train_samples_per_class.items()}\n",
        "\n",
        "print(f'Labels:         {labels_file}')\n",
        "print(f'Embeddings dir: {embeddings_dir}')\n",
        "print(f'Output:         {output_file}')\n",
        "print(f'\\nTest region:  tiles with col_offset < {TEST_COL_MAX} (left ~22% of map)')\n",
        "print(f'Train region: tiles with col_offset >= {TEST_COL_MAX} (right ~78%)')\n",
        "print(f'\\nTrain target: {sum(train_samples_per_class.values()):,} samples')\n",
        "print(f'Test target:  {sum(test_samples_per_class.values()):,} samples')\n",
        "\n",
        "assert os.path.exists(labels_file), 'Labels TIF not found'\n",
        "assert embeddings_dir.exists(), 'Embeddings dir not found'\n",
        "\n",
        "print('\\nConfiguration validated!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: Discover tiles and split into train / test sets by column\n",
        "print('=' * 70)\n",
        "print('DISCOVERING TILES AND SPLITTING (COLUMN-BASED)')\n",
        "print('=' * 70)\n",
        "\n",
        "all_tile_files = sorted(embeddings_dir.glob('*.tif'))\n",
        "print(f'Found {len(all_tile_files)} total tiles')\n",
        "\n",
        "# Parse row/col offsets from filenames: *-RRRRRRRRRR-CCCCCCCCCC.tif\n",
        "tile_info = []\n",
        "for tf in all_tile_files:\n",
        "    parts = tf.stem.split('-')\n",
        "    if len(parts) >= 3:\n",
        "        try:\n",
        "            row_off = int(parts[-2])\n",
        "            col_off = int(parts[-1])\n",
        "            tile_info.append((row_off, col_off, tf))\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "print(f'Parseable tiles: {len(tile_info)}')\n",
        "\n",
        "# Split: test = left columns (col_off < TEST_COL_MAX), train = rest\n",
        "test_tiles  = [p for r, c, p in tile_info if c < TEST_COL_MAX]\n",
        "train_tiles = [p for r, c, p in tile_info if c >= TEST_COL_MAX]\n",
        "\n",
        "all_col_offsets = sorted(set(c for r, c, p in tile_info))\n",
        "col_min = all_col_offsets[0]\n",
        "col_max = all_col_offsets[-1]\n",
        "\n",
        "print(f'\\nColumn range of all tiles: {col_min} — {col_max}')\n",
        "print(f'TEST_COL_MAX threshold:    {TEST_COL_MAX}')\n",
        "print(f'Train tiles: {len(train_tiles)}')\n",
        "print(f'Test tiles:  {len(test_tiles)}')\n",
        "print(f'Test fraction: {len(test_tiles) / len(tile_info) * 100:.1f}% of all tiles')\n",
        "\n",
        "if not test_tiles:\n",
        "    raise RuntimeError('No test tiles found — increase TEST_COL_MAX or check tile naming.')\n",
        "\n",
        "print('\\nTest tiles (left column band):')\n",
        "for p in test_tiles:\n",
        "    print(f'  {p.name}')\n",
        "\n",
        "print('\\nSpatial split defined!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Sample pixel coordinates restricted to each split's bounding box\n",
        "print('\\n' + '=' * 70)\n",
        "print('SAMPLING PIXEL COORDINATES')\n",
        "print('=' * 70)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def sample_coords_from_tiles(tile_paths, samples_per_class, split_name):\n",
        "    \"\"\"Sample pixel coordinates only within the bounding box of the given tiles.\"\"\"\n",
        "    if not tile_paths:\n",
        "        return {cls: {'y': [], 'x': []} for cls in samples_per_class}, {cls: 0 for cls in samples_per_class}\n",
        "\n",
        "    # Compute bounding box of these tiles in raster pixel coords\n",
        "    tile_info_local = []\n",
        "    for tf in tile_paths:\n",
        "        parts = tf.stem.split('-')\n",
        "        try:\n",
        "            r = int(parts[-2]); c = int(parts[-1])\n",
        "            with rasterio.open(tf) as src:\n",
        "                tile_info_local.append((r, c, src.height, src.width))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    bbox_row_min = min(r for r, c, h, w in tile_info_local)\n",
        "    bbox_row_max = max(r + h for r, c, h, w in tile_info_local)\n",
        "    bbox_col_min = min(c for r, c, h, w in tile_info_local)\n",
        "    bbox_col_max = max(c + w for r, c, h, w in tile_info_local)\n",
        "\n",
        "    print(f'\\n  {split_name} tile bounding box: rows {bbox_row_min}–{bbox_row_max}, cols {bbox_col_min}–{bbox_col_max}')\n",
        "\n",
        "    sampled   = {cls: {'y': [], 'x': []} for cls in samples_per_class}\n",
        "    collected = {cls: 0 for cls in samples_per_class}\n",
        "\n",
        "    with rasterio.open(labels_file) as src:\n",
        "        windows = list(src.block_windows(1))\n",
        "        np.random.shuffle(windows)\n",
        "\n",
        "        for idx, (block_id, window) in tqdm(enumerate(windows), total=len(windows), desc=f'{split_name} blocks'):\n",
        "            row_off = window.row_off\n",
        "            col_off = window.col_off\n",
        "            win_h   = window.height\n",
        "            win_w   = window.width\n",
        "\n",
        "            # Skip blocks outside this split's tile bounding box\n",
        "            if row_off + win_h <= bbox_row_min or row_off >= bbox_row_max:\n",
        "                continue\n",
        "            if col_off + win_w <= bbox_col_min or col_off >= bbox_col_max:\n",
        "                continue\n",
        "\n",
        "            labels_chunk = src.read(1, window=window)\n",
        "\n",
        "            for cls in samples_per_class:\n",
        "                if collected[cls] >= samples_per_class[cls]:\n",
        "                    continue\n",
        "                y_local, x_local = np.where(labels_chunk == cls)\n",
        "                if len(y_local) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_global = y_local + row_off\n",
        "                x_global = x_local + col_off\n",
        "\n",
        "                # Keep only pixels inside this split's bbox\n",
        "                in_bbox = (\n",
        "                    (y_global >= bbox_row_min) & (y_global < bbox_row_max) &\n",
        "                    (x_global >= bbox_col_min) & (x_global < bbox_col_max)\n",
        "                )\n",
        "                y_global = y_global[in_bbox]\n",
        "                x_global = x_global[in_bbox]\n",
        "                if len(y_global) == 0:\n",
        "                    continue\n",
        "\n",
        "                needed = samples_per_class[cls] - collected[cls]\n",
        "                if len(y_global) > needed:\n",
        "                    idx_s = np.random.choice(len(y_global), needed, replace=False)\n",
        "                    y_global = y_global[idx_s]\n",
        "                    x_global = x_global[idx_s]\n",
        "\n",
        "                sampled[cls]['y'].append(y_global)\n",
        "                sampled[cls]['x'].append(x_global)\n",
        "                collected[cls] += len(y_global)\n",
        "\n",
        "            if all(collected[cls] >= samples_per_class[cls] for cls in samples_per_class):\n",
        "                print(f'\\n  Got all {split_name} samples after {idx+1} blocks')\n",
        "                break\n",
        "\n",
        "    print(f'  {split_name} collection summary:')\n",
        "    for cls in samples_per_class:\n",
        "        print(f'    Class {cls}: {collected[cls]:,} / {samples_per_class[cls]:,}')\n",
        "\n",
        "    return sampled, collected\n",
        "\n",
        "\n",
        "train_sampled, train_collected = sample_coords_from_tiles(train_tiles, train_samples_per_class, 'TRAIN')\n",
        "test_sampled,  test_collected  = sample_coords_from_tiles(test_tiles,  test_samples_per_class,  'TEST')\n",
        "\n",
        "print('\\nCoordinate sampling complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Consolidate sampled coords into flat arrays\n",
        "\n",
        "def consolidate(sampled, samples_per_class):\n",
        "    all_y, all_x, all_labels = [], [], []\n",
        "    for cls in samples_per_class:\n",
        "        if not sampled[cls]['y']:\n",
        "            continue\n",
        "        ys = np.concatenate(sampled[cls]['y'])\n",
        "        xs = np.concatenate(sampled[cls]['x'])\n",
        "        if len(ys) > samples_per_class[cls]:\n",
        "            ys = ys[:samples_per_class[cls]]\n",
        "            xs = xs[:samples_per_class[cls]]\n",
        "        all_y.append(ys)\n",
        "        all_x.append(xs)\n",
        "        all_labels.append(np.full(len(ys), cls))\n",
        "    y_idx  = np.concatenate(all_y)\n",
        "    x_idx  = np.concatenate(all_x)\n",
        "    labels = np.concatenate(all_labels)\n",
        "    shuf = np.random.permutation(len(labels))\n",
        "    return y_idx[shuf], x_idx[shuf], labels[shuf]\n",
        "\n",
        "\n",
        "train_y_idx, train_x_idx, train_labels = consolidate(train_sampled, train_samples_per_class)\n",
        "test_y_idx,  test_x_idx,  test_labels  = consolidate(test_sampled,  test_samples_per_class)\n",
        "\n",
        "print(f'Train coordinates: {len(train_labels):,}')\n",
        "print(f'Test  coordinates: {len(test_labels):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Extract embeddings for each split\n",
        "print('\\n' + '=' * 70)\n",
        "print('EXTRACTING EMBEDDINGS')\n",
        "print('=' * 70)\n",
        "\n",
        "def extract_embeddings(tile_files, y_indices, x_indices, desc):\n",
        "    n = len(y_indices)\n",
        "    X = np.zeros((n, 64), dtype=np.float32)\n",
        "    found = np.zeros(n, dtype=bool)\n",
        "\n",
        "    with tqdm(total=len(tile_files), desc=desc, unit=' tiles') as pbar:\n",
        "        for tile_file in tile_files:\n",
        "            try:\n",
        "                with rasterio.open(tile_file) as src:\n",
        "                    if src.count != 64:\n",
        "                        pbar.update(1); continue\n",
        "\n",
        "                    parts = tile_file.stem.split('-')\n",
        "                    try:\n",
        "                        r_off = int(parts[-2])\n",
        "                        c_off = int(parts[-1])\n",
        "                    except (ValueError, IndexError):\n",
        "                        pbar.update(1); continue\n",
        "\n",
        "                    th, tw = src.height, src.width\n",
        "                    in_y = (y_indices >= r_off) & (y_indices < r_off + th)\n",
        "                    in_x = (x_indices >= c_off) & (x_indices < c_off + tw)\n",
        "                    mask = in_y & in_x\n",
        "\n",
        "                    if mask.any():\n",
        "                        tile_data = src.read()  # (64, H, W)\n",
        "                        if tile_data.shape[0] != 64:\n",
        "                            pbar.update(1); continue\n",
        "\n",
        "                        local_y = y_indices[mask] - r_off\n",
        "                        local_x = x_indices[mask] - c_off\n",
        "                        vals = tile_data[:, local_y, local_x].T  # (n_px, 64)\n",
        "\n",
        "                        valid = ~np.isnan(vals).any(axis=1)\n",
        "                        g_idx = np.where(mask)[0]\n",
        "                        X[g_idx[valid]] = vals[valid]\n",
        "                        found[g_idx[valid]] = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'\\nError {tile_file.name}: {e}')\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({'found': f'{found.sum():,}/{n:,}'})\n",
        "\n",
        "    print(f'  Extracted {found.sum():,} / {n:,}')\n",
        "    return X, found\n",
        "\n",
        "\n",
        "print('\\n-- TRAIN --')\n",
        "X_train_raw, train_found = extract_embeddings(train_tiles, train_y_idx, train_x_idx, 'Train tiles')\n",
        "X_train = X_train_raw[train_found]\n",
        "y_train = train_labels[train_found]\n",
        "\n",
        "print('\\n-- TEST --')\n",
        "X_test_raw, test_found = extract_embeddings(test_tiles, test_y_idx, test_x_idx, 'Test tiles')\n",
        "X_test = X_test_raw[test_found]\n",
        "y_test = test_labels[test_found]\n",
        "\n",
        "print(f'\\nFinal train set: {X_train.shape}')\n",
        "print(f'Final test set:  {X_test.shape}')\n",
        "print('Extraction complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Compute class weights and save\n",
        "print('\\n' + '=' * 70)\n",
        "print('SAVING DATASET')\n",
        "print('=' * 70)\n",
        "\n",
        "unique_cls, counts = np.unique(y_train, return_counts=True)\n",
        "class_weights = np.zeros(6, dtype=np.float32)\n",
        "for cls, cnt in zip(unique_cls, counts):\n",
        "    class_weights[cls] = 1.0 / cnt\n",
        "class_weights = class_weights / class_weights.sum() * 6\n",
        "\n",
        "print('Class weights (from train):')\n",
        "for cls in range(6):\n",
        "    print(f'  Class {cls}: {class_weights[cls]:.4f}')\n",
        "\n",
        "print(f'\\nSaving to: {output_file}')\n",
        "np.savez_compressed(\n",
        "    output_file,\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    class_weights=class_weights,\n",
        "    test_col_max=np.array(TEST_COL_MAX, dtype=np.int64),\n",
        ")\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('DATASET SAVED: wetland_dataset_smart_split.npz')\n",
        "print('=' * 70)\n",
        "print(f'  X_train: {X_train.shape}  (train pixels — right ~78% of map)')\n",
        "print(f'  y_train: {y_train.shape}')\n",
        "print(f'  X_test:  {X_test.shape}   (test pixels — left ~22%, guaranteed all-class)')\n",
        "print(f'  y_test:  {y_test.shape}')\n",
        "print(f'  test_col_max: {TEST_COL_MAX}')\n",
        "print(f'\\nNext steps:')\n",
        "print(f'  1. Download wetland_dataset_smart_split.npz from Google Drive')\n",
        "print(f'  2. Place it in the repo root (same level as random_forest/)')\n",
        "print(f'  3. Run: python random_forest_spatial/model_rf_spatial.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: Quick verification\n",
        "print('\\n' + '=' * 70)\n",
        "print('VERIFICATION')\n",
        "print('=' * 70)\n",
        "\n",
        "d = np.load(output_file)\n",
        "print(f'Arrays: {list(d.keys())}')\n",
        "print(f'X_train: {d[\"X_train\"].shape}  dtype={d[\"X_train\"].dtype}  NaN={np.isnan(d[\"X_train\"]).any()}')\n",
        "print(f'X_test:  {d[\"X_test\"].shape}   dtype={d[\"X_test\"].dtype}   NaN={np.isnan(d[\"X_test\"]).any()}')\n",
        "print(f'y_train classes: {np.unique(d[\"y_train\"])}')\n",
        "print(f'y_test  classes: {np.unique(d[\"y_test\"])}')\n",
        "print(f'test_col_max:    {int(d[\"test_col_max\"])}')\n",
        "d.close()\n",
        "\n",
        "print('\\nVerification passed!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
