{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Wetland Training Dataset — Spatial Split\n",
                "\n",
                "**Output:** `wetland_dataset_spatial_split.npz`\n",
                "\n",
                "This notebook fixes the **spatial data leakage** problem in the original dataset creator.\n",
                "Instead of a random pixel-level 80/20 split, the dataset is split **by tile region**:\n",
                "- Training pixels are sampled only from tiles in the top ~80% of the study area (by row offset)\n",
                "- Test pixels are sampled only from tiles in the bottom ~20% (a contiguous geographic block the model never sees during training)\n",
                "\n",
                "This produces a geographically honest evaluation — the model is tested on a region it has genuinely never encountered."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 1: Setup\n",
                "print('Setting up environment...')\n",
                "\n",
                "import os\n",
                "from google.colab import drive\n",
                "\n",
                "if not os.path.exists('/content/drive'):\n",
                "    drive.mount('/content/drive')\n",
                "else:\n",
                "    print('Drive already mounted')\n",
                "\n",
                "!pip install -q rasterio tqdm\n",
                "\n",
                "import numpy as np\n",
                "import rasterio\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "\n",
                "print('Setup complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 2: Configuration\n",
                "print('=' * 70)\n",
                "print('CONFIGURATION')\n",
                "print('=' * 70)\n",
                "\n",
                "labels_file    = '/content/drive/MyDrive/bow_river_wetlands_10m_final.tif'\n",
                "embeddings_dir = Path('/content/drive/MyDrive/EarthEngine')\n",
                "output_file    = '/content/drive/MyDrive/wetland_dataset_spatial_split.npz'\n",
                "\n",
                "# ──────────────────────────────────────────────────────────────\n",
                "# SPATIAL SPLIT: what fraction of tiles (by row offset) to hold\n",
                "# out as the test region. 0.20 = bottom 20% of tile rows.\n",
                "TEST_TILE_FRACTION = 0.20\n",
                "# ──────────────────────────────────────────────────────────────\n",
                "\n",
                "# Per-class sample budgets (training pixels only)\n",
                "train_samples_per_class = {\n",
                "    0: 600_000,\n",
                "    1: 19_225,\n",
                "    2: 150_000,\n",
                "    3: 500_000,\n",
                "    4: 150_000,\n",
                "    5: 100_000,\n",
                "}\n",
                "# Test budget: ~20% of training budget\n",
                "test_samples_per_class = {cls: max(1000, int(n * 0.25)) for cls, n in train_samples_per_class.items()}\n",
                "\n",
                "print(f'Labels:     {labels_file}')\n",
                "print(f'Embeddings: {embeddings_dir}')\n",
                "print(f'Output:     {output_file}')\n",
                "print(f'Test tile fraction: {TEST_TILE_FRACTION*100:.0f}% of row range held out')\n",
                "print(f'\\nTrain target: {sum(train_samples_per_class.values()):,} samples')\n",
                "print(f'Test target:  {sum(test_samples_per_class.values()):,} samples')\n",
                "\n",
                "assert os.path.exists(labels_file), 'Labels not found'\n",
                "assert embeddings_dir.exists(), 'Embeddings dir not found'\n",
                "\n",
                "print('Configuration validated!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 3: Discover tiles and split into train / test sets\n",
                "print('=' * 70)\n",
                "print('DISCOVERING TILES AND SPLITTING SPATIALLY')\n",
                "print('=' * 70)\n",
                "\n",
                "all_tile_files = sorted(embeddings_dir.glob('*.tif'))\n",
                "print(f'Found {len(all_tile_files)} total tiles')\n",
                "\n",
                "# Parse row offsets from filenames: *-RRRRRRRRRR-CCCCCCCCCC.tif\n",
                "tile_info = []  # list of (row_offset, col_offset, path)\n",
                "for tf in all_tile_files:\n",
                "    parts = tf.stem.split('-')\n",
                "    if len(parts) >= 3:\n",
                "        try:\n",
                "            row_off = int(parts[-2])\n",
                "            col_off = int(parts[-1])\n",
                "            tile_info.append((row_off, col_off, tf))\n",
                "        except ValueError:\n",
                "            pass\n",
                "\n",
                "print(f'Parseable tiles: {len(tile_info)}')\n",
                "\n",
                "# Determine the cutoff row: top TEST_TILE_FRACTION of the row range becomes test\n",
                "all_row_offsets = sorted(set(r for r, c, p in tile_info))\n",
                "row_min = all_row_offsets[0]\n",
                "row_max = all_row_offsets[-1]\n",
                "total_row_range = row_max - row_min + 1\n",
                "cutoff_rows_from_bottom = int(total_row_range * TEST_TILE_FRACTION)\n",
                "test_row_min = row_max - cutoff_rows_from_bottom + 1\n",
                "\n",
                "train_tiles = [p for r, c, p in tile_info if r < test_row_min]\n",
                "test_tiles  = [p for r, c, p in tile_info if r >= test_row_min]\n",
                "\n",
                "print(f'\\nRow range: {row_min} – {row_max} (range = {total_row_range})')\n",
                "print(f'Test region: rows >= {test_row_min}  (bottom {TEST_TILE_FRACTION*100:.0f}% of range)')\n",
                "print(f'Train tiles: {len(train_tiles)}')\n",
                "print(f'Test tiles:  {len(test_tiles)}')\n",
                "\n",
                "if not test_tiles:\n",
                "    raise RuntimeError('No test tiles found — increase TEST_TILE_FRACTION or check tile naming.')\n",
                "\n",
                "print('\\nTest tiles:')\n",
                "for p in test_tiles:\n",
                "    print(f'  {p.name}')\n",
                "\n",
                "print('\\nSpatial split defined!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 4: Sample coordinates from labels raster, restricted per split\n",
                "print('\\n' + '=' * 70)\n",
                "print('SAMPLING PIXEL COORDINATES')\n",
                "print('=' * 70)\n",
                "\n",
                "np.random.seed(42)\n",
                "\n",
                "def sample_coords_from_tiles(tile_paths, samples_per_class, split_name):\n",
                "    \"\"\"Sample pixel coordinates only within the bounding box of the given tiles.\"\"\"\n",
                "    if not tile_paths:\n",
                "        return {cls: {'y': [], 'x': []} for cls in samples_per_class}, {cls: 0 for cls in samples_per_class}\n",
                "\n",
                "    # Compute bounding box of these tiles (in raster pixel coords)\n",
                "    tile_info_local = []\n",
                "    for tf in tile_paths:\n",
                "        parts = tf.stem.split('-')\n",
                "        try:\n",
                "            r = int(parts[-2]); c = int(parts[-1])\n",
                "            with rasterio.open(tf) as src:\n",
                "                tile_info_local.append((r, c, src.height, src.width))\n",
                "        except Exception:\n",
                "            pass\n",
                "\n",
                "    row_starts = [r for r, c, h, w in tile_info_local]\n",
                "    row_ends   = [r + h for r, c, h, w in tile_info_local]\n",
                "    col_starts = [c for r, c, h, w in tile_info_local]\n",
                "    col_ends   = [c + w for r, c, h, w in tile_info_local]\n",
                "\n",
                "    bbox_row_min = min(row_starts)\n",
                "    bbox_row_max = max(row_ends)\n",
                "    bbox_col_min = min(col_starts)\n",
                "    bbox_col_max = max(col_ends)\n",
                "\n",
                "    print(f'\\n  {split_name} tile bounding box: rows {bbox_row_min}–{bbox_row_max}, cols {bbox_col_min}–{bbox_col_max}')\n",
                "\n",
                "    sampled = {cls: {'y': [], 'x': []} for cls in samples_per_class}\n",
                "    collected = {cls: 0 for cls in samples_per_class}\n",
                "\n",
                "    with rasterio.open(labels_file) as src:\n",
                "        windows = list(src.block_windows(1))\n",
                "        np.random.shuffle(windows)\n",
                "\n",
                "        for idx, (block_id, window) in tqdm(enumerate(windows), total=len(windows), desc=f'{split_name} blocks'):\n",
                "            row_off = window.row_off\n",
                "            col_off = window.col_off\n",
                "            win_h   = window.height\n",
                "            win_w   = window.width\n",
                "\n",
                "            # Skip blocks outside this split's tile bounding box\n",
                "            if row_off + win_h <= bbox_row_min or row_off >= bbox_row_max:\n",
                "                continue\n",
                "            if col_off + win_w <= bbox_col_min or col_off >= bbox_col_max:\n",
                "                continue\n",
                "\n",
                "            labels_chunk = src.read(1, window=window)\n",
                "\n",
                "            for cls in samples_per_class:\n",
                "                if collected[cls] >= samples_per_class[cls]:\n",
                "                    continue\n",
                "                y_local, x_local = np.where(labels_chunk == cls)\n",
                "                if len(y_local) == 0:\n",
                "                    continue\n",
                "\n",
                "                y_global = y_local + row_off\n",
                "                x_global = x_local + col_off\n",
                "\n",
                "                # Keep only pixels inside tile bounding box\n",
                "                in_bbox = (\n",
                "                    (y_global >= bbox_row_min) & (y_global < bbox_row_max) &\n",
                "                    (x_global >= bbox_col_min) & (x_global < bbox_col_max)\n",
                "                )\n",
                "                y_global = y_global[in_bbox]\n",
                "                x_global = x_global[in_bbox]\n",
                "                if len(y_global) == 0:\n",
                "                    continue\n",
                "\n",
                "                needed = samples_per_class[cls] - collected[cls]\n",
                "                if len(y_global) > needed:\n",
                "                    idx_s = np.random.choice(len(y_global), needed, replace=False)\n",
                "                    y_global = y_global[idx_s]\n",
                "                    x_global = x_global[idx_s]\n",
                "\n",
                "                sampled[cls]['y'].append(y_global)\n",
                "                sampled[cls]['x'].append(x_global)\n",
                "                collected[cls] += len(y_global)\n",
                "\n",
                "            if all(collected[cls] >= samples_per_class[cls] for cls in samples_per_class):\n",
                "                print(f'\\n  Got all {split_name} samples after {idx+1} blocks')\n",
                "                break\n",
                "\n",
                "    print(f'  {split_name} collection summary:')\n",
                "    for cls in samples_per_class:\n",
                "        print(f'    Class {cls}: {collected[cls]:,} / {samples_per_class[cls]:,}')\n",
                "\n",
                "    return sampled, collected\n",
                "\n",
                "\n",
                "train_sampled, train_collected = sample_coords_from_tiles(train_tiles, train_samples_per_class, 'TRAIN')\n",
                "test_sampled,  test_collected  = sample_coords_from_tiles(test_tiles,  test_samples_per_class,  'TEST')\n",
                "\n",
                "print('\\nCoordinate sampling complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 5: Helper — consolidate sampled coords into flat arrays\n",
                "\n",
                "def consolidate(sampled, samples_per_class):\n",
                "    all_y, all_x, all_labels = [], [], []\n",
                "    for cls in samples_per_class:\n",
                "        if not sampled[cls]['y']:\n",
                "            continue\n",
                "        ys = np.concatenate(sampled[cls]['y'])\n",
                "        xs = np.concatenate(sampled[cls]['x'])\n",
                "        if len(ys) > samples_per_class[cls]:\n",
                "            ys = ys[:samples_per_class[cls]]\n",
                "            xs = xs[:samples_per_class[cls]]\n",
                "        all_y.append(ys)\n",
                "        all_x.append(xs)\n",
                "        all_labels.append(np.full(len(ys), cls))\n",
                "\n",
                "    y_idx  = np.concatenate(all_y)\n",
                "    x_idx  = np.concatenate(all_x)\n",
                "    labels = np.concatenate(all_labels)\n",
                "\n",
                "    shuf = np.random.permutation(len(labels))\n",
                "    return y_idx[shuf], x_idx[shuf], labels[shuf]\n",
                "\n",
                "train_y_idx, train_x_idx, train_labels = consolidate(train_sampled, train_samples_per_class)\n",
                "test_y_idx,  test_x_idx,  test_labels  = consolidate(test_sampled,  test_samples_per_class)\n",
                "\n",
                "print(f'Train coordinates: {len(train_labels):,}')\n",
                "print(f'Test  coordinates: {len(test_labels):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 6: Extract embeddings for each split\n",
                "print('\\n' + '=' * 70)\n",
                "print('EXTRACTING EMBEDDINGS')\n",
                "print('=' * 70)\n",
                "\n",
                "def extract_embeddings(tile_files, y_indices, x_indices, desc):\n",
                "    n = len(y_indices)\n",
                "    X = np.zeros((n, 64), dtype=np.float32)\n",
                "    found = np.zeros(n, dtype=bool)\n",
                "\n",
                "    with tqdm(total=len(tile_files), desc=desc, unit=' tiles') as pbar:\n",
                "        for tile_file in tile_files:\n",
                "            try:\n",
                "                with rasterio.open(tile_file) as src:\n",
                "                    if src.count != 64:\n",
                "                        pbar.update(1); continue\n",
                "\n",
                "                    parts = tile_file.stem.split('-')\n",
                "                    try:\n",
                "                        r_off = int(parts[-2])\n",
                "                        c_off = int(parts[-1])\n",
                "                    except (ValueError, IndexError):\n",
                "                        pbar.update(1); continue\n",
                "\n",
                "                    th, tw = src.height, src.width\n",
                "                    in_y = (y_indices >= r_off) & (y_indices < r_off + th)\n",
                "                    in_x = (x_indices >= c_off) & (x_indices < c_off + tw)\n",
                "                    mask = in_y & in_x\n",
                "\n",
                "                    if mask.any():\n",
                "                        tile_data = src.read()  # (64, H, W)\n",
                "                        if tile_data.shape[0] != 64:\n",
                "                            pbar.update(1); continue\n",
                "\n",
                "                        local_y = y_indices[mask] - r_off\n",
                "                        local_x = x_indices[mask] - c_off\n",
                "                        vals = tile_data[:, local_y, local_x].T  # (n_px, 64)\n",
                "\n",
                "                        valid = ~np.isnan(vals).any(axis=1)\n",
                "                        g_idx = np.where(mask)[0]\n",
                "                        X[g_idx[valid]] = vals[valid]\n",
                "                        found[g_idx[valid]] = True\n",
                "\n",
                "            except Exception as e:\n",
                "                print(f'\\nError {tile_file.name}: {e}')\n",
                "            pbar.update(1)\n",
                "            pbar.set_postfix({'found': f'{found.sum():,}/{n:,}'})\n",
                "\n",
                "    print(f'  Extracted {found.sum():,} / {n:,}')\n",
                "    return X, found\n",
                "\n",
                "\n",
                "print('\\n-- TRAIN --')\n",
                "X_train_raw, train_found = extract_embeddings(train_tiles, train_y_idx, train_x_idx, 'Train tiles')\n",
                "X_train = X_train_raw[train_found]\n",
                "y_train = train_labels[train_found]\n",
                "\n",
                "print('\\n-- TEST --')\n",
                "X_test_raw, test_found = extract_embeddings(test_tiles,  test_y_idx,  test_x_idx,  'Test tiles')\n",
                "X_test = X_test_raw[test_found]\n",
                "y_test = test_labels[test_found]\n",
                "\n",
                "print(f'\\nFinal train set: {X_train.shape}')\n",
                "print(f'Final test set:  {X_test.shape}')\n",
                "print('Extraction complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 7: Compute class weights and save\n",
                "print('\\n' + '=' * 70)\n",
                "print('SAVING DATASET')\n",
                "print('=' * 70)\n",
                "\n",
                "# Class weights from training distribution only\n",
                "unique_cls, counts = np.unique(y_train, return_counts=True)\n",
                "class_weights = np.zeros(6, dtype=np.float32)\n",
                "for cls, cnt in zip(unique_cls, counts):\n",
                "    class_weights[cls] = 1.0 / cnt\n",
                "class_weights = class_weights / class_weights.sum() * 6  # normalize\n",
                "\n",
                "print('Class weights (from train):')\n",
                "for cls in range(6):\n",
                "    print(f'  Class {cls}: {class_weights[cls]:.4f}')\n",
                "\n",
                "print(f'\\nSaving to: {output_file}')\n",
                "np.savez_compressed(\n",
                "    output_file,\n",
                "    X_train=X_train,\n",
                "    y_train=y_train,\n",
                "    X_test=X_test,\n",
                "    y_test=y_test,\n",
                "    class_weights=class_weights,\n",
                "    test_row_min=np.array(test_row_min, dtype=np.int64),\n",
                ")\n",
                "\n",
                "print('\\n' + '=' * 70)\n",
                "print('DATASET SAVED: wetland_dataset_spatial_split.npz')\n",
                "print('=' * 70)\n",
                "print(f'  X_train: {X_train.shape}  (train pixels from geographic top ~80%)')\n",
                "print(f'  y_train: {y_train.shape}')\n",
                "print(f'  X_test:  {X_test.shape}   (test pixels from geographic bottom ~20%)')\n",
                "print(f'  y_test:  {y_test.shape}')\n",
                "print(f'  test_row_min: {test_row_min}  (use this in visualize_test_region.py)')\n",
                "print(f'\\nNext steps:')\n",
                "print(f'  1. Download wetland_dataset_spatial_split.npz from Google Drive')\n",
                "print(f'  2. Place it in the repo root (same level as random_forest/)')\n",
                "print(f'  3. Run: python random_forest/model_rf_spatial.py')\n",
                "print(f'  4. Run: python random_forest/visualize_test_region.py <embeddings_dir>')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 8: Quick verification\n",
                "print('\\n' + '=' * 70)\n",
                "print('VERIFICATION')\n",
                "print('=' * 70)\n",
                "\n",
                "d = np.load(output_file)\n",
                "print(f'Arrays: {list(d.keys())}')\n",
                "print(f'X_train: {d[\"X_train\"].shape}  dtype={d[\"X_train\"].dtype}  NaN={np.isnan(d[\"X_train\"]).any()}')\n",
                "print(f'X_test:  {d[\"X_test\"].shape}   dtype={d[\"X_test\"].dtype}   NaN={np.isnan(d[\"X_test\"]).any()}')\n",
                "print(f'y_train classes: {np.unique(d[\"y_train\"])}')\n",
                "print(f'y_test  classes: {np.unique(d[\"y_test\"])}')\n",
                "print(f'test_row_min: {int(d[\"test_row_min\"])}')\n",
                "d.close()\n",
                "\n",
                "print('\\nVerification passed!')"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}