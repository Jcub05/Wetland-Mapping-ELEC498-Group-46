{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wetland Training Dataset — Smart Spatial Split\n",
        "\n",
        "**Output:** `wetland_dataset_smart_split.npz`\n",
        "\n",
        "## Split Strategy\n",
        "\n",
        "A **mixed split** is used because Class 1 (only 19,225 pixels total) is entirely\n",
        "confined to the western portion of the study area and cannot be geographically split:\n",
        "\n",
        "| Class | Split type | Rationale |\n",
        "|-------|----------|----------|\n",
        "| 0, 2, 3, 4, 5 | **Geographic** — left/right column tile split | Pixels spread across entire map |\n",
        "| **1** | **Random 75/25 within its zone** | All 19,225 pixels in cols 1000–6311; no geo split possible |\n",
        "\n",
        "> **Documented limitation:** Class 1 train/test split is not geographically independent.\n",
        "> This should be noted in the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1: Setup\n",
        "import os, gc\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('Drive already mounted')\n",
        "\n",
        "!pip install -q rasterio tqdm\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check disk space — tiles are ~1.2 GB each × 88 = ~106 GB total.\n",
        "# Only attempt local copy if there is enough space.\n",
        "import shutil\n",
        "free_gb = shutil.disk_usage('/content').free / (1024**3)\n",
        "print(f'Free disk space: {free_gb:.1f} GB')\n",
        "DRIVE_TILES  = '/content/drive/MyDrive/EarthEngine'\n",
        "LOCAL_TILES  = '/content/EarthEngine'\n",
        "\n",
        "if free_gb > 110 and not os.path.exists(LOCAL_TILES):\n",
        "    print('Enough space — copying tiles to local disk for faster I/O (~10 min)...')\n",
        "    !cp -r {DRIVE_TILES} {LOCAL_TILES}\n",
        "    TILES_PATH = LOCAL_TILES\n",
        "    print('Copy done! Using local tiles.')\n",
        "elif os.path.exists(LOCAL_TILES) and len(list(Path(LOCAL_TILES).glob('*.tif'))) > 0:\n",
        "    TILES_PATH = LOCAL_TILES\n",
        "    print(f'Local tiles already present — using {LOCAL_TILES}')\n",
        "else:\n",
        "    TILES_PATH = DRIVE_TILES\n",
        "    print(f'Not enough disk space ({free_gb:.1f} GB) — reading tiles from Drive.')\n",
        "    print('Extraction will take ~60-90 min (Drive I/O bound). This is expected.')\n",
        "\n",
        "print(f'\\nTile source: {TILES_PATH}')\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Configuration\n",
        "print('=' * 70)\n",
        "print('CONFIGURATION')\n",
        "print('=' * 70)\n",
        "\n",
        "labels_file    = '/content/drive/MyDrive/bow_river_wetlands_10m_final.tif'\n",
        "embeddings_dir = Path(TILES_PATH)\n",
        "output_file    = '/content/drive/MyDrive/wetland_dataset_smart_split.npz'\n",
        "\n",
        "TEST_COL_MAX = 8192\n",
        "\n",
        "CLASS1_TRAIN_FRACTION = 0.75\n",
        "CLASS1_ROW_MIN, CLASS1_ROW_MAX = 764,  15197\n",
        "CLASS1_COL_MIN, CLASS1_COL_MAX = 1000,  6311\n",
        "\n",
        "train_samples_per_class = {\n",
        "    0: 600_000,\n",
        "    1: 19_225,\n",
        "    2: 150_000,\n",
        "    3: 500_000,\n",
        "    4: 150_000,\n",
        "    5: 100_000,\n",
        "}\n",
        "test_samples_per_class = {cls: max(1000, int(n * 0.25)) for cls, n in train_samples_per_class.items()}\n",
        "train_samples_geo = {cls: n for cls, n in train_samples_per_class.items() if cls != 1}\n",
        "test_samples_geo  = {cls: n for cls, n in test_samples_per_class.items()  if cls != 1}\n",
        "\n",
        "assert os.path.exists(labels_file), f'Labels TIF not found: {labels_file}'\n",
        "assert embeddings_dir.exists(),     f'Embeddings dir not found: {embeddings_dir}'\n",
        "tile_count = len(list(embeddings_dir.glob('*.tif')))\n",
        "assert tile_count > 0, f'No .tif tiles found in {embeddings_dir}'\n",
        "\n",
        "print(f'Labels:        {labels_file}')\n",
        "print(f'Embeddings:    {embeddings_dir}  ({tile_count} tiles found)')\n",
        "print(f'Output:        {output_file}')\n",
        "print(f'TEST_COL_MAX:  {TEST_COL_MAX}')\n",
        "print(f'Train target (geo): {sum(train_samples_geo.values()):,}')\n",
        "print(f'Test target  (geo): {sum(test_samples_geo.values()):,}')\n",
        "print('Configuration validated!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: Discover tiles and do column-based geographic split\n",
        "print('=' * 70)\n",
        "print('DISCOVERING TILES — COLUMN SPLIT')\n",
        "print('=' * 70)\n",
        "\n",
        "all_tile_files = sorted(embeddings_dir.glob('*.tif'))\n",
        "print(f'Found {len(all_tile_files)} total tiles')\n",
        "\n",
        "tile_info = []\n",
        "for tf in all_tile_files:\n",
        "    parts = tf.stem.split('-')\n",
        "    if len(parts) >= 3:\n",
        "        try:\n",
        "            tile_info.append((int(parts[-2]), int(parts[-1]), tf))\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "print(f'Parseable tiles: {len(tile_info)}')\n",
        "if len(tile_info) == 0:\n",
        "    raise RuntimeError('No tiles could be parsed — check that tile filenames contain row/col offsets like *-RRRR-CCCC.tif')\n",
        "\n",
        "test_tiles  = [p for r, c, p in tile_info if c < TEST_COL_MAX]\n",
        "train_tiles = [p for r, c, p in tile_info if c >= TEST_COL_MAX]\n",
        "\n",
        "print(f'Train tiles (eastern, col >= {TEST_COL_MAX}): {len(train_tiles)}')\n",
        "print(f'Test  tiles (western, col <  {TEST_COL_MAX}): {len(test_tiles)}')\n",
        "print(f'Test fraction: {len(test_tiles)/len(tile_info)*100:.1f}% of all tiles')\n",
        "\n",
        "if not test_tiles:\n",
        "    raise RuntimeError('No test tiles found — check TEST_COL_MAX')\n",
        "print('\\nSpatial split defined!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Sample Class 1 separately (random 75/25 within its bounding zone)\n",
        "print('=' * 70)\n",
        "print('SAMPLING CLASS 1 (RANDOM SPLIT WITHIN ZONE)')\n",
        "print('=' * 70)\n",
        "\n",
        "np.random.seed(42)\n",
        "class1_y_all, class1_x_all = [], []\n",
        "with rasterio.open(labels_file) as src:\n",
        "    for block_id, window in tqdm(list(src.block_windows(1)), desc='Scanning for Class 1'):\n",
        "        r0 = window.row_off; c0 = window.col_off\n",
        "        if r0 + window.height <= CLASS1_ROW_MIN or r0 > CLASS1_ROW_MAX: continue\n",
        "        if c0 + window.width  <= CLASS1_COL_MIN or c0 > CLASS1_COL_MAX: continue\n",
        "        chunk = src.read(1, window=window)\n",
        "        y_loc, x_loc = np.where(chunk == 1)\n",
        "        if len(y_loc) == 0: continue\n",
        "        class1_y_all.append(y_loc + r0)\n",
        "        class1_x_all.append(x_loc + c0)\n",
        "\n",
        "class1_y = np.concatenate(class1_y_all)\n",
        "class1_x = np.concatenate(class1_x_all)\n",
        "del class1_y_all, class1_x_all; gc.collect()\n",
        "\n",
        "shuf = np.random.permutation(len(class1_y))\n",
        "class1_y = class1_y[shuf]; class1_x = class1_x[shuf]; del shuf\n",
        "\n",
        "n_train1 = int(len(class1_y) * CLASS1_TRAIN_FRACTION)\n",
        "class1_train_y = class1_y[:n_train1].copy()\n",
        "class1_train_x = class1_x[:n_train1].copy()\n",
        "class1_test_y  = class1_y[n_train1:].copy()\n",
        "class1_test_x  = class1_x[n_train1:].copy()\n",
        "del class1_y, class1_x; gc.collect()\n",
        "\n",
        "print(f'Total Class 1 pixels: {n_train1 + len(class1_test_y):,}')\n",
        "print(f'  Train: {len(class1_train_y):,}  |  Test: {len(class1_test_y):,}')\n",
        "print('\\n⚠ NOTE: Class 1 split is NOT geographically independent (documented limitation).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Sample geographic classes (0, 2, 3, 4, 5) using tile-based column split\n",
        "print('=' * 70)\n",
        "print('SAMPLING GEOGRAPHIC CLASSES (0, 2, 3, 4, 5)')\n",
        "print('=' * 70)\n",
        "\n",
        "def sample_coords_from_tiles(tile_paths, samples_per_class, split_name):\n",
        "    if not tile_paths:\n",
        "        raise RuntimeError(f'No tiles provided for {split_name}. Check tile discovery in Cell 3.')\n",
        "    tile_info_local = []\n",
        "    for tf in tile_paths:\n",
        "        parts = tf.stem.split('-')\n",
        "        try:\n",
        "            r = int(parts[-2]); c = int(parts[-1])\n",
        "            with rasterio.open(tf) as s:\n",
        "                tile_info_local.append((r, c, s.height, s.width))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not tile_info_local:\n",
        "        raise RuntimeError(f'{split_name}: could not open any tiles. Check file paths and naming.')\n",
        "\n",
        "    bbox_row_min = min(r     for r,c,h,w in tile_info_local)\n",
        "    bbox_row_max = max(r+h   for r,c,h,w in tile_info_local)\n",
        "    bbox_col_min = min(c     for r,c,h,w in tile_info_local)\n",
        "    bbox_col_max = max(c+w   for r,c,h,w in tile_info_local)\n",
        "    print(f'  {split_name} bbox: rows {bbox_row_min}–{bbox_row_max}, cols {bbox_col_min}–{bbox_col_max}')\n",
        "\n",
        "    sampled   = {cls: {'y': [], 'x': []} for cls in samples_per_class}\n",
        "    collected = {cls: 0 for cls in samples_per_class}\n",
        "\n",
        "    with rasterio.open(labels_file) as src:\n",
        "        windows = list(src.block_windows(1))\n",
        "        np.random.shuffle(windows)\n",
        "        for idx, (block_id, window) in tqdm(enumerate(windows), total=len(windows), desc=f'{split_name}'):\n",
        "            r0 = window.row_off; c0 = window.col_off\n",
        "            rh = window.height;  cw = window.width\n",
        "            if r0+rh <= bbox_row_min or r0 >= bbox_row_max: continue\n",
        "            if c0+cw <= bbox_col_min or c0 >= bbox_col_max: continue\n",
        "            chunk = src.read(1, window=window)\n",
        "            for cls in samples_per_class:\n",
        "                if collected[cls] >= samples_per_class[cls]: continue\n",
        "                y_l, x_l = np.where(chunk == cls)\n",
        "                if len(y_l) == 0: continue\n",
        "                y_g = y_l + r0; x_g = x_l + c0\n",
        "                in_b = (y_g>=bbox_row_min)&(y_g<bbox_row_max)&(x_g>=bbox_col_min)&(x_g<bbox_col_max)\n",
        "                y_g = y_g[in_b]; x_g = x_g[in_b]\n",
        "                if len(y_g) == 0: continue\n",
        "                needed = samples_per_class[cls] - collected[cls]\n",
        "                if len(y_g) > needed:\n",
        "                    s = np.random.choice(len(y_g), needed, replace=False)\n",
        "                    y_g = y_g[s]; x_g = x_g[s]\n",
        "                sampled[cls]['y'].append(y_g)\n",
        "                sampled[cls]['x'].append(x_g)\n",
        "                collected[cls] += len(y_g)\n",
        "            if all(collected[c] >= samples_per_class[c] for c in samples_per_class):\n",
        "                print(f'  All samples collected after {idx+1} blocks'); break\n",
        "    print(f'  {split_name} summary:')\n",
        "    for cls in samples_per_class:\n",
        "        print(f'    Class {cls}: {collected[cls]:,} / {samples_per_class[cls]:,}')\n",
        "    return sampled\n",
        "\n",
        "train_sampled_geo = sample_coords_from_tiles(train_tiles, train_samples_geo, 'TRAIN (geo)')\n",
        "test_sampled_geo  = sample_coords_from_tiles(test_tiles,  test_samples_geo,  'TEST (geo)')\n",
        "print('\\nGeographic class sampling complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Consolidate all coordinates; aggressively free intermediate memory\n",
        "\n",
        "def consolidate_geo(sampled, budgets):\n",
        "    all_y, all_x, all_lbl = [], [], []\n",
        "    for cls in budgets:\n",
        "        if not sampled[cls]['y']: continue\n",
        "        ys = np.concatenate(sampled[cls]['y'])\n",
        "        xs = np.concatenate(sampled[cls]['x'])\n",
        "        if len(ys) > budgets[cls]: ys, xs = ys[:budgets[cls]], xs[:budgets[cls]]\n",
        "        all_y.append(ys); all_x.append(xs)\n",
        "        all_lbl.append(np.full(len(ys), cls, dtype=np.int64))\n",
        "    return np.concatenate(all_y), np.concatenate(all_x), np.concatenate(all_lbl)\n",
        "\n",
        "geo_train_y, geo_train_x, geo_train_lbl = consolidate_geo(train_sampled_geo, train_samples_geo)\n",
        "del train_sampled_geo; gc.collect()\n",
        "\n",
        "geo_test_y, geo_test_x, geo_test_lbl = consolidate_geo(test_sampled_geo, test_samples_geo)\n",
        "del test_sampled_geo; gc.collect()\n",
        "\n",
        "train_y = np.concatenate([geo_train_y, class1_train_y])\n",
        "train_x = np.concatenate([geo_train_x, class1_train_x])\n",
        "train_labels = np.concatenate([geo_train_lbl, np.ones(len(class1_train_y), dtype=np.int64)])\n",
        "del geo_train_y, geo_train_x, geo_train_lbl, class1_train_y, class1_train_x; gc.collect()\n",
        "\n",
        "test_y = np.concatenate([geo_test_y,  class1_test_y])\n",
        "test_x = np.concatenate([geo_test_x,  class1_test_x])\n",
        "test_labels = np.concatenate([geo_test_lbl, np.ones(len(class1_test_y), dtype=np.int64)])\n",
        "del geo_test_y, geo_test_x, geo_test_lbl, class1_test_y, class1_test_x; gc.collect()\n",
        "\n",
        "shuf = np.random.permutation(len(train_labels))\n",
        "train_y, train_x, train_labels = train_y[shuf], train_x[shuf], train_labels[shuf]; del shuf\n",
        "shuf = np.random.permutation(len(test_labels))\n",
        "test_y, test_x, test_labels = test_y[shuf], test_x[shuf], test_labels[shuf]\n",
        "del shuf; gc.collect()\n",
        "\n",
        "print(f'Train coordinates: {len(train_labels):,}  classes: {np.unique(train_labels)}')\n",
        "print(f'Test  coordinates: {len(test_labels):,}   classes: {np.unique(test_labels)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Extract embeddings\n",
        "# Tile reads peak at ~1.2 GB each. Persistent arrays peak at ~450 MB.\n",
        "# del statements prevent OOM between train and test extraction.\n",
        "print('\\n' + '=' * 70)\n",
        "print('EXTRACTING EMBEDDINGS')\n",
        "print('=' * 70)\n",
        "\n",
        "def extract_embeddings(tile_files, y_indices, x_indices, desc):\n",
        "    n = len(y_indices)\n",
        "    X = np.zeros((n, 64), dtype=np.float32)\n",
        "    found = np.zeros(n, dtype=bool)\n",
        "    with tqdm(total=len(tile_files), desc=desc, unit=' tiles') as pbar:\n",
        "        for tf in tile_files:\n",
        "            try:\n",
        "                with rasterio.open(tf) as src:\n",
        "                    if src.count != 64: pbar.update(1); continue\n",
        "                    parts = tf.stem.split('-')\n",
        "                    try: r_off = int(parts[-2]); c_off = int(parts[-1])\n",
        "                    except: pbar.update(1); continue\n",
        "                    th, tw = src.height, src.width\n",
        "                    mask = ((y_indices>=r_off)&(y_indices<r_off+th)&\n",
        "                            (x_indices>=c_off)&(x_indices<c_off+tw))\n",
        "                    if mask.any():\n",
        "                        tile_data = src.read()\n",
        "                        if tile_data.shape[0] != 64: pbar.update(1); continue\n",
        "                        ly = y_indices[mask] - r_off\n",
        "                        lx = x_indices[mask] - c_off\n",
        "                        vals = tile_data[:, ly, lx].T\n",
        "                        del tile_data  # free 1.2 GB tile immediately\n",
        "                        valid = ~np.isnan(vals).any(axis=1)\n",
        "                        g_idx = np.where(mask)[0]\n",
        "                        X[g_idx[valid]] = vals[valid]\n",
        "                        found[g_idx[valid]] = True\n",
        "                        del vals\n",
        "            except Exception as e:\n",
        "                print(f'\\nError {tf.name}: {e}')\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({'found': f'{found.sum():,}/{n:,}'})\n",
        "    print(f'  Extracted {found.sum():,} / {n:,}')\n",
        "    return X, found\n",
        "\n",
        "\n",
        "print('\\n-- TRAIN --')\n",
        "X_train_raw, train_found = extract_embeddings(all_tile_files, train_y, train_x, 'Train tiles')\n",
        "X_train = X_train_raw[train_found]\n",
        "y_train = train_labels[train_found]\n",
        "del X_train_raw, train_found, train_y, train_x, train_labels; gc.collect()  # free ~360 MB before test\n",
        "print(f'X_train: {X_train.shape}')\n",
        "\n",
        "print('\\n-- TEST --')\n",
        "X_test_raw, test_found = extract_embeddings(all_tile_files, test_y, test_x, 'Test tiles')\n",
        "X_test = X_test_raw[test_found]\n",
        "y_test = test_labels[test_found]\n",
        "del X_test_raw, test_found, test_y, test_x, test_labels; gc.collect()  # free before save\n",
        "print(f'X_test: {X_test.shape}')\n",
        "\n",
        "print('\\nExtraction complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: Compute class weights and save to Drive\n",
        "print('\\n' + '=' * 70)\n",
        "print('SAVING DATASET')\n",
        "print('=' * 70)\n",
        "\n",
        "unique_cls, counts = np.unique(y_train, return_counts=True)\n",
        "class_weights = np.zeros(6, dtype=np.float32)\n",
        "for cls, cnt in zip(unique_cls, counts):\n",
        "    class_weights[cls] = 1.0 / cnt\n",
        "class_weights = class_weights / class_weights.sum() * 6\n",
        "\n",
        "print('Class weights (from train):')\n",
        "for cls in range(6):\n",
        "    print(f'  Class {cls}: {class_weights[cls]:.4f}')\n",
        "\n",
        "np.savez_compressed(\n",
        "    output_file,\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    X_test=X_test,   y_test=y_test,\n",
        "    class_weights=class_weights,\n",
        "    test_col_max=np.array(TEST_COL_MAX, dtype=np.int64),\n",
        ")\n",
        "print(f'\\nSaved: {output_file}')\n",
        "print(f'  X_train: {X_train.shape}  |  X_test: {X_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: Verification\n",
        "d = np.load(output_file)\n",
        "print(f'Arrays: {list(d.keys())}')\n",
        "print(f'X_train: {d[\"X_train\"].shape}  NaN={np.isnan(d[\"X_train\"]).any()}')\n",
        "print(f'X_test:  {d[\"X_test\"].shape}   NaN={np.isnan(d[\"X_test\"]).any()}')\n",
        "print(f'y_train classes: {np.unique(d[\"y_train\"])}')\n",
        "print(f'y_test  classes: {np.unique(d[\"y_test\"])}')\n",
        "d.close()\n",
        "print('Verification passed!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}