{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wetland Training Dataset — Smart Spatial Split\n",
        "\n",
        "**Output:** `wetland_dataset_smart_split.npz`\n",
        "\n",
        "## Split Strategy\n",
        "\n",
        "Classes 1 and 2 are both confined to the western portion of the map and cannot be\n",
        "geographically split. A **mixed split** is used:\n",
        "\n",
        "| Class | Split type | Rationale |\n",
        "|-------|----------|----------|\n",
        "| 0, 3, 4, 5 | **Geographic** — left/right column tile split | Pixels spread across entire map |\n",
        "| **1** | **Random 75/25 within zone** | All 19,225 pixels in cols 1,000–6,311 |\n",
        "| **2** | **Random 75/25 within zone** | All 901,620 pixels in cols 747–7,037 |\n",
        "\n",
        "> **Documented limitation:** Classes 1 and 2 train/test splits are not geographically\n",
        "> independent. This should be noted in the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1: Setup\n",
        "import os, gc, shutil\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('Drive already mounted')\n",
        "\n",
        "!pip install -q rasterio tqdm\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "DRIVE_TILES = '/content/drive/MyDrive/EarthEngine'\n",
        "LOCAL_TILES  = '/content/EarthEngine'\n",
        "free_gb = shutil.disk_usage('/content').free / (1024**3)\n",
        "print(f'Free disk space: {free_gb:.1f} GB')\n",
        "\n",
        "if free_gb > 110 and not os.path.exists(LOCAL_TILES):\n",
        "    print('Copying tiles to local disk (~10 min)...')\n",
        "    !cp -r {DRIVE_TILES} {LOCAL_TILES}\n",
        "    TILES_PATH = LOCAL_TILES\n",
        "elif os.path.exists(LOCAL_TILES) and len(list(Path(LOCAL_TILES).glob('*.tif'))) > 0:\n",
        "    TILES_PATH = LOCAL_TILES\n",
        "    print('Local tiles already present.')\n",
        "else:\n",
        "    TILES_PATH = DRIVE_TILES\n",
        "    print(f'Insufficient disk ({free_gb:.1f} GB) — reading from Drive. Extraction will take ~60-90 min.')\n",
        "\n",
        "print(f'Tile source: {TILES_PATH}')\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Configuration\n",
        "print('=' * 70)\n",
        "print('CONFIGURATION')\n",
        "print('=' * 70)\n",
        "\n",
        "labels_file    = '/content/drive/MyDrive/bow_river_wetlands_10m_final.tif'\n",
        "embeddings_dir = Path(TILES_PATH)\n",
        "output_file    = '/content/drive/MyDrive/wetland_dataset_smart_split.npz'\n",
        "\n",
        "# Geographic split threshold (classes 0, 3, 4, 5 only)\n",
        "TEST_COL_MAX = 8192\n",
        "\n",
        "# Classes to handle with random within-zone split (both are confined to western region)\n",
        "WESTERN_CLASSES = {\n",
        "    1: {'row_min': 764,  'row_max': 15197, 'col_min': 1000, 'col_max': 6311,  'train_budget': 14_418, 'test_budget': 4_806},\n",
        "    2: {'row_min': 45,   'row_max': 15175, 'col_min': 747,  'col_max': 7037,  'train_budget': 150_000, 'test_budget': 37_500},\n",
        "}\n",
        "\n",
        "# Geographic classes (split by tile column)\n",
        "train_samples_geo = {0: 600_000, 3: 500_000, 4: 150_000, 5: 100_000}\n",
        "test_samples_geo  = {cls: max(1000, int(n * 0.25)) for cls, n in train_samples_geo.items()}\n",
        "\n",
        "assert os.path.exists(labels_file), f'Labels TIF not found: {labels_file}'\n",
        "assert embeddings_dir.exists(),     f'Embeddings dir not found: {embeddings_dir}'\n",
        "tile_count = len(list(embeddings_dir.glob('*.tif')))\n",
        "assert tile_count > 0, f'No .tif files in {embeddings_dir}'\n",
        "\n",
        "print(f'Labels:     {labels_file}')\n",
        "print(f'Embeddings: {embeddings_dir}  ({tile_count} tiles)')\n",
        "print(f'Output:     {output_file}')\n",
        "print(f'TEST_COL_MAX: {TEST_COL_MAX}')\n",
        "print(f'Western classes (random split): {list(WESTERN_CLASSES.keys())}')\n",
        "print(f'Geographic classes: {list(train_samples_geo.keys())}')\n",
        "print('Configuration validated!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: Discover tiles and do column-based geographic split\n",
        "print('=' * 70)\n",
        "print('DISCOVERING TILES — COLUMN SPLIT')\n",
        "print('=' * 70)\n",
        "\n",
        "all_tile_files = sorted(embeddings_dir.glob('*.tif'))\n",
        "tile_info = []\n",
        "for tf in all_tile_files:\n",
        "    parts = tf.stem.split('-')\n",
        "    if len(parts) >= 3:\n",
        "        try: tile_info.append((int(parts[-2]), int(parts[-1]), tf))\n",
        "        except ValueError: pass\n",
        "\n",
        "if not tile_info:\n",
        "    raise RuntimeError('No parseable tiles found — check tile naming (*-ROW-COL.tif)')\n",
        "\n",
        "test_tiles  = [p for r, c, p in tile_info if c < TEST_COL_MAX]\n",
        "train_tiles = [p for r, c, p in tile_info if c >= TEST_COL_MAX]\n",
        "\n",
        "print(f'Total tiles: {len(tile_info)}')\n",
        "print(f'Train tiles (col >= {TEST_COL_MAX}): {len(train_tiles)}')\n",
        "print(f'Test  tiles (col <  {TEST_COL_MAX}): {len(test_tiles)}')\n",
        "print(f'Test fraction: {len(test_tiles)/len(tile_info)*100:.1f}% of all tiles')\n",
        "\n",
        "if not test_tiles:\n",
        "    raise RuntimeError('No test tiles found — check TEST_COL_MAX')\n",
        "print('Spatial split defined!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Sample Classes 1 and 2 (random split within their western zones)\n",
        "print('=' * 70)\n",
        "print('SAMPLING WESTERN CLASSES (1, 2) — RANDOM WITHIN-ZONE SPLIT')\n",
        "print('=' * 70)\n",
        "np.random.seed(42)\n",
        "\n",
        "western_train_coords = {}  # cls -> (y, x)\n",
        "western_test_coords  = {}\n",
        "\n",
        "for cls, cfg in WESTERN_CLASSES.items():\n",
        "    print(f'\\n--- Class {cls} (zone: rows {cfg[\"row_min\"]}–{cfg[\"row_max\"]}, cols {cfg[\"col_min\"]}–{cfg[\"col_max\"]}) ---')\n",
        "    y_all, x_all = [], []\n",
        "\n",
        "    with rasterio.open(labels_file) as src:\n",
        "        for block_id, window in tqdm(list(src.block_windows(1)), desc=f'Class {cls} scan'):\n",
        "            r0 = window.row_off; c0 = window.col_off\n",
        "            if r0 + window.height <= cfg['row_min'] or r0 > cfg['row_max']: continue\n",
        "            if c0 + window.width  <= cfg['col_min'] or c0 > cfg['col_max']: continue\n",
        "            chunk = src.read(1, window=window)\n",
        "            y_l, x_l = np.where(chunk == cls)\n",
        "            if len(y_l) == 0: continue\n",
        "            y_all.append(y_l + r0)\n",
        "            x_all.append(x_l + c0)\n",
        "\n",
        "    y_cls = np.concatenate(y_all); x_cls = np.concatenate(x_all)\n",
        "    del y_all, x_all; gc.collect()\n",
        "    print(f'  Found {len(y_cls):,} pixels')\n",
        "\n",
        "    # Subsample to budget total then split\n",
        "    total_budget = cfg['train_budget'] + cfg['test_budget']\n",
        "    if len(y_cls) > total_budget:\n",
        "        idx = np.random.choice(len(y_cls), total_budget, replace=False)\n",
        "        y_cls = y_cls[idx]; x_cls = x_cls[idx]\n",
        "\n",
        "    shuf = np.random.permutation(len(y_cls))\n",
        "    y_cls = y_cls[shuf]; x_cls = x_cls[shuf]; del shuf\n",
        "\n",
        "    n_train = min(cfg['train_budget'], len(y_cls))\n",
        "    western_train_coords[cls] = (y_cls[:n_train].copy(), x_cls[:n_train].copy())\n",
        "    western_test_coords[cls]  = (y_cls[n_train:n_train + cfg['test_budget']].copy(),\n",
        "                                  x_cls[n_train:n_train + cfg['test_budget']].copy())\n",
        "    del y_cls, x_cls; gc.collect()\n",
        "\n",
        "    print(f'  Train: {len(western_train_coords[cls][0]):,}  |  Test: {len(western_test_coords[cls][0]):,}')\n",
        "\n",
        "print('\\n⚠ NOTE: Classes 1 and 2 splits are NOT geographically independent (documented limitation).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Sample geographic classes (0, 3, 4, 5) using tile-based column split\n",
        "print('=' * 70)\n",
        "print('SAMPLING GEOGRAPHIC CLASSES (0, 3, 4, 5)')\n",
        "print('=' * 70)\n",
        "\n",
        "def sample_coords_from_tiles(tile_paths, samples_per_class, split_name):\n",
        "    if not tile_paths:\n",
        "        raise RuntimeError(f'No tiles provided for {split_name}.')\n",
        "    tile_info_local = []\n",
        "    for tf in tile_paths:\n",
        "        parts = tf.stem.split('-')\n",
        "        try:\n",
        "            r = int(parts[-2]); c = int(parts[-1])\n",
        "            with rasterio.open(tf) as s:\n",
        "                tile_info_local.append((r, c, s.height, s.width))\n",
        "        except Exception: pass\n",
        "    if not tile_info_local:\n",
        "        raise RuntimeError(f'{split_name}: could not open any tiles.')\n",
        "\n",
        "    bbox_row_min = min(r   for r,c,h,w in tile_info_local)\n",
        "    bbox_row_max = max(r+h for r,c,h,w in tile_info_local)\n",
        "    bbox_col_min = min(c   for r,c,h,w in tile_info_local)\n",
        "    bbox_col_max = max(c+w for r,c,h,w in tile_info_local)\n",
        "    print(f'  {split_name} bbox: rows {bbox_row_min}–{bbox_row_max}, cols {bbox_col_min}–{bbox_col_max}')\n",
        "\n",
        "    sampled   = {cls: {'y': [], 'x': []} for cls in samples_per_class}\n",
        "    collected = {cls: 0 for cls in samples_per_class}\n",
        "\n",
        "    with rasterio.open(labels_file) as src:\n",
        "        windows = list(src.block_windows(1))\n",
        "        np.random.shuffle(windows)\n",
        "        for idx, (block_id, window) in tqdm(enumerate(windows), total=len(windows), desc=split_name):\n",
        "            r0 = window.row_off; c0 = window.col_off\n",
        "            rh = window.height;  cw = window.width\n",
        "            if r0+rh <= bbox_row_min or r0 >= bbox_row_max: continue\n",
        "            if c0+cw <= bbox_col_min or c0 >= bbox_col_max: continue\n",
        "            chunk = src.read(1, window=window)\n",
        "            for cls in samples_per_class:\n",
        "                if collected[cls] >= samples_per_class[cls]: continue\n",
        "                y_l, x_l = np.where(chunk == cls)\n",
        "                if len(y_l) == 0: continue\n",
        "                y_g = y_l + r0; x_g = x_l + c0\n",
        "                in_b = (y_g>=bbox_row_min)&(y_g<bbox_row_max)&(x_g>=bbox_col_min)&(x_g<bbox_col_max)\n",
        "                y_g = y_g[in_b]; x_g = x_g[in_b]\n",
        "                if len(y_g) == 0: continue\n",
        "                needed = samples_per_class[cls] - collected[cls]\n",
        "                if len(y_g) > needed:\n",
        "                    s = np.random.choice(len(y_g), needed, replace=False)\n",
        "                    y_g = y_g[s]; x_g = x_g[s]\n",
        "                sampled[cls]['y'].append(y_g)\n",
        "                sampled[cls]['x'].append(x_g)\n",
        "                collected[cls] += len(y_g)\n",
        "            if all(collected[c] >= samples_per_class[c] for c in samples_per_class):\n",
        "                print(f'  All collected after {idx+1} blocks'); break\n",
        "    print(f'  {split_name} summary:')\n",
        "    for cls in samples_per_class:\n",
        "        print(f'    Class {cls}: {collected[cls]:,} / {samples_per_class[cls]:,}')\n",
        "    return sampled\n",
        "\n",
        "train_sampled_geo = sample_coords_from_tiles(train_tiles, train_samples_geo, 'TRAIN (geo)')\n",
        "test_sampled_geo  = sample_coords_from_tiles(test_tiles,  test_samples_geo,  'TEST (geo)')\n",
        "print('\\nGeographic class sampling complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Consolidate all coordinates; aggressively free intermediate memory\n",
        "\n",
        "def consolidate_geo(sampled, budgets):\n",
        "    all_y, all_x, all_lbl = [], [], []\n",
        "    for cls in budgets:\n",
        "        if not sampled[cls]['y']: continue\n",
        "        ys = np.concatenate(sampled[cls]['y'])\n",
        "        xs = np.concatenate(sampled[cls]['x'])\n",
        "        if len(ys) > budgets[cls]: ys, xs = ys[:budgets[cls]], xs[:budgets[cls]]\n",
        "        all_y.append(ys); all_x.append(xs)\n",
        "        all_lbl.append(np.full(len(ys), cls, dtype=np.int64))\n",
        "    return np.concatenate(all_y), np.concatenate(all_x), np.concatenate(all_lbl)\n",
        "\n",
        "geo_tr_y, geo_tr_x, geo_tr_lbl = consolidate_geo(train_sampled_geo, train_samples_geo)\n",
        "del train_sampled_geo; gc.collect()\n",
        "geo_te_y, geo_te_x, geo_te_lbl = consolidate_geo(test_sampled_geo,  test_samples_geo)\n",
        "del test_sampled_geo; gc.collect()\n",
        "\n",
        "# Build western class arrays\n",
        "w_tr_y   = np.concatenate([western_train_coords[c][0] for c in WESTERN_CLASSES])\n",
        "w_tr_x   = np.concatenate([western_train_coords[c][1] for c in WESTERN_CLASSES])\n",
        "w_tr_lbl = np.concatenate([np.full(len(western_train_coords[c][0]), c, dtype=np.int64) for c in WESTERN_CLASSES])\n",
        "w_te_y   = np.concatenate([western_test_coords[c][0]  for c in WESTERN_CLASSES])\n",
        "w_te_x   = np.concatenate([western_test_coords[c][1]  for c in WESTERN_CLASSES])\n",
        "w_te_lbl = np.concatenate([np.full(len(western_test_coords[c][0]),  c, dtype=np.int64) for c in WESTERN_CLASSES])\n",
        "del western_train_coords, western_test_coords; gc.collect()\n",
        "\n",
        "train_y = np.concatenate([geo_tr_y, w_tr_y])\n",
        "train_x = np.concatenate([geo_tr_x, w_tr_x])\n",
        "train_labels = np.concatenate([geo_tr_lbl, w_tr_lbl])\n",
        "del geo_tr_y, geo_tr_x, geo_tr_lbl, w_tr_y, w_tr_x, w_tr_lbl; gc.collect()\n",
        "\n",
        "test_y = np.concatenate([geo_te_y, w_te_y])\n",
        "test_x = np.concatenate([geo_te_x, w_te_x])\n",
        "test_labels = np.concatenate([geo_te_lbl, w_te_lbl])\n",
        "del geo_te_y, geo_te_x, geo_te_lbl, w_te_y, w_te_x, w_te_lbl; gc.collect()\n",
        "\n",
        "shuf = np.random.permutation(len(train_labels))\n",
        "train_y, train_x, train_labels = train_y[shuf], train_x[shuf], train_labels[shuf]; del shuf\n",
        "shuf = np.random.permutation(len(test_labels))\n",
        "test_y, test_x, test_labels = test_y[shuf], test_x[shuf], test_labels[shuf]\n",
        "del shuf; gc.collect()\n",
        "\n",
        "print(f'Train: {len(train_labels):,}  classes: {sorted(np.unique(train_labels).tolist())}')\n",
        "print(f'Test:  {len(test_labels):,}   classes: {sorted(np.unique(test_labels).tolist())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Extract embeddings — del raw buffers between train/test to prevent OOM\n",
        "print('\\n' + '=' * 70)\n",
        "print('EXTRACTING EMBEDDINGS')\n",
        "print('=' * 70)\n",
        "\n",
        "def extract_embeddings(tile_files, y_indices, x_indices, desc):\n",
        "    n = len(y_indices)\n",
        "    X = np.zeros((n, 64), dtype=np.float32)\n",
        "    found = np.zeros(n, dtype=bool)\n",
        "    with tqdm(total=len(tile_files), desc=desc, unit=' tiles') as pbar:\n",
        "        for tf in tile_files:\n",
        "            try:\n",
        "                with rasterio.open(tf) as src:\n",
        "                    if src.count != 64: pbar.update(1); continue\n",
        "                    parts = tf.stem.split('-')\n",
        "                    try: r_off = int(parts[-2]); c_off = int(parts[-1])\n",
        "                    except: pbar.update(1); continue\n",
        "                    th, tw = src.height, src.width\n",
        "                    mask = ((y_indices>=r_off)&(y_indices<r_off+th)&\n",
        "                            (x_indices>=c_off)&(x_indices<c_off+tw))\n",
        "                    if mask.any():\n",
        "                        tile_data = src.read()\n",
        "                        if tile_data.shape[0] != 64: pbar.update(1); continue\n",
        "                        ly = y_indices[mask] - r_off\n",
        "                        lx = x_indices[mask] - c_off\n",
        "                        vals = tile_data[:, ly, lx].T\n",
        "                        del tile_data\n",
        "                        valid = ~np.isnan(vals).any(axis=1)\n",
        "                        g_idx = np.where(mask)[0]\n",
        "                        X[g_idx[valid]] = vals[valid]\n",
        "                        found[g_idx[valid]] = True\n",
        "                        del vals\n",
        "            except Exception as e:\n",
        "                print(f'\\nError {tf.name}: {e}')\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({'found': f'{found.sum():,}/{n:,}'})\n",
        "    print(f'  Extracted {found.sum():,} / {n:,}')\n",
        "    return X, found\n",
        "\n",
        "print('\\n-- TRAIN --')\n",
        "X_train_raw, train_found = extract_embeddings(all_tile_files, train_y, train_x, 'Train tiles')\n",
        "X_train = X_train_raw[train_found]\n",
        "y_train = train_labels[train_found]\n",
        "del X_train_raw, train_found, train_y, train_x, train_labels; gc.collect()\n",
        "print(f'X_train: {X_train.shape}  classes: {sorted(np.unique(y_train).tolist())}')\n",
        "\n",
        "print('\\n-- TEST --')\n",
        "X_test_raw, test_found = extract_embeddings(all_tile_files, test_y, test_x, 'Test tiles')\n",
        "X_test = X_test_raw[test_found]\n",
        "y_test = test_labels[test_found]\n",
        "del X_test_raw, test_found, test_y, test_x, test_labels; gc.collect()\n",
        "print(f'X_test: {X_test.shape}  classes: {sorted(np.unique(y_test).tolist())}')\n",
        "\n",
        "print('\\nExtraction complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: Compute class weights and save\n",
        "print('\\n' + '=' * 70)\n",
        "print('SAVING DATASET')\n",
        "print('=' * 70)\n",
        "\n",
        "unique_cls, counts = np.unique(y_train, return_counts=True)\n",
        "class_weights = np.zeros(6, dtype=np.float32)\n",
        "for cls, cnt in zip(unique_cls, counts):\n",
        "    class_weights[cls] = 1.0 / cnt\n",
        "class_weights = class_weights / class_weights.sum() * 6\n",
        "\n",
        "print('Class weights (from train):')\n",
        "for cls in range(6):\n",
        "    print(f'  Class {cls}: {class_weights[cls]:.4f}')\n",
        "\n",
        "np.savez_compressed(\n",
        "    output_file,\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    X_test=X_test,   y_test=y_test,\n",
        "    class_weights=class_weights,\n",
        "    test_col_max=np.array(TEST_COL_MAX, dtype=np.int64),\n",
        ")\n",
        "print(f'\\nSaved: {output_file}')\n",
        "print(f'  X_train: {X_train.shape}  |  X_test: {X_test.shape}')\n",
        "print('\\nNext steps:')\n",
        "print('  1. Download wetland_dataset_smart_split.npz from Drive')\n",
        "print('  2. Place in repo root')\n",
        "print('  3. Run: python random_forest_spatial/model_rf_spatial.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: Verification\n",
        "d = np.load(output_file)\n",
        "print(f'Arrays: {list(d.keys())}')\n",
        "print(f'X_train: {d[\"X_train\"].shape}  NaN={np.isnan(d[\"X_train\"]).any()}')\n",
        "print(f'X_test:  {d[\"X_test\"].shape}   NaN={np.isnan(d[\"X_test\"]).any()}')\n",
        "print(f'y_train classes: {sorted(np.unique(d[\"y_train\"]).tolist())}')\n",
        "print(f'y_test  classes: {sorted(np.unique(d[\"y_test\"]).tolist())}')\n",
        "d.close()\n",
        "print('Verification passed!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}